#!/usr/bin/env python3
"""
Scraping Control Panel Demo
Demonstrates the complete web scraping functionality
"""

import asyncio
import json
from pathlib import Path
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from services.scraping.job_manager import (
    ScrapingJobManager, ScrapingType, JobPriority, JobStatus
)
from services.scraping.scheduler import ScrapingScheduler

async def demo_scraping_control_panel():
    """Demo the scraping control panel functionality"""
    print("\n" + "="*80)
    print("üï∑Ô∏è SCRAPING CONTROL PANEL DEMO")
    print("="*80)
    print("Demonstrating web data collection and intelligence gathering")
    print("="*80)
    
    # Initialize managers
    print("\nüöÄ Initializing Scraping System...")
    job_manager = ScrapingJobManager()
    scheduler = ScrapingScheduler(job_manager)
    
    # Show initial statistics
    stats = job_manager.get_job_statistics()
    print(f"   üìä Current Jobs: {stats['total_jobs']}")
    print(f"   üèÉ Running Jobs: {stats['running_jobs']}")
    print(f"   üìà Total Executions: {stats['total_executions']}")
    
    # Create a sample job
    print(f"\n‚ûï Creating Sample Scraping Job...")
    
    sample_urls = [
        "https://httpbin.org/json",
        "https://httpbin.org/headers",
        "https://httpbin.org/user-agent"
    ]
    
    job_id = job_manager.create_job(
        name="Demo Data Collection",
        job_type=ScrapingType.CUSTOM,
        target_urls=sample_urls,
        priority=JobPriority.HIGH,
        config={
            'delay': 2.0,
            'max_content_length': 5000,
            'timeout': 15
        },
        metadata={
            'purpose': 'demonstration',
            'created_by': 'demo_script'
        }
    )
    
    print(f"   ‚úÖ Created job: {job_id}")
    
    # Execute the job
    print(f"\n‚ñ∂Ô∏è Executing Scraping Job...")
    async with job_manager:
        try:
            execution_id = await job_manager.execute_job(job_id)
            print(f"   üèÉ Started execution: {execution_id}")
            
            # Wait for completion (with timeout)
            import time
            start_time = time.time()
            while time.time() - start_time < 30:  # 30 second timeout
                job = job_manager.get_job(job_id)
                if job.status in [JobStatus.COMPLETED, JobStatus.FAILED]:
                    break
                await asyncio.sleep(1)
            
            # Show results
            job = job_manager.get_job(job_id)
            print(f"   üìä Job Status: {job.status.value}")
            print(f"   üìà Progress: {job.progress:.1f}%")
            print(f"   ‚úÖ Successful: {job.successful_extractions}")
            print(f"   ‚ùå Failed: {job.failed_extractions}")
            print(f"   üì¶ Data Extracted: {job.data_extracted}")
            
            if job.output_path:
                print(f"   üìÅ Output: {job.output_path}")
        
        except Exception as e:
            print(f"   ‚ùå Execution error: {e}")
    
    # Show scheduler capabilities
    print(f"\nüìÖ Scheduler Demonstration...")
    
    # Start scheduler
    scheduler.start()
    print(f"   ‚úÖ Scheduler started")
    
    # Schedule the job
    scheduler.schedule_job(job_id, "interval", interval=5)  # Every 5 minutes
    print(f"   ‚è∞ Job scheduled for every 5 minutes")
    
    # Show scheduled jobs
    scheduled = scheduler.get_scheduled_jobs()
    print(f"   üìã Scheduled jobs: {len(scheduled)}")
    
    # Stop scheduler
    scheduler.stop()
    print(f"   ‚èπÔ∏è Scheduler stopped")
    
    # Show execution history
    print(f"\nüìà Execution History...")
    executions = job_manager.get_execution_history(job_id)
    
    for i, execution in enumerate(executions[:3], 1):  # Show first 3
        print(f"   {i}. {execution.execution_id[:12]}... - {execution.status.value}")
        print(f"      Started: {execution.started_at[:19] if execution.started_at else 'N/A'}")
        print(f"      URLs: {execution.urls_processed}, Data: {execution.data_extracted}")
    
    # Show system capabilities
    print(f"\nüéØ System Capabilities:")
    print(f"   üï∑Ô∏è Multi-source web scraping")
    print(f"   üìÖ Advanced job scheduling (cron, interval, daily, weekly)")
    print(f"   üìä Real-time progress monitoring")
    print(f"   üìà Performance analytics and reporting")
    print(f"   üö® Error handling and retry mechanisms")
    print(f"   üì¶ Structured data extraction and storage")
    print(f"   ‚öôÔ∏è Configurable delays and request settings")
    print(f"   üîÑ Background execution and queue management")
    
    # Show data types supported
    print(f"\nüìã Supported Data Sources:")
    for scraping_type in ScrapingType:
        print(f"   ‚Ä¢ {scraping_type.value.replace('_', ' ').title()}")
    
    # Platform integration
    print(f"\nüîó Platform Integration:")
    print(f"   ‚úÖ Integrated with Compliant.one dashboard")
    print(f"   ‚úÖ Permission-based access control")
    print(f"   ‚úÖ Real-time monitoring and alerts")
    print(f"   ‚úÖ Export capabilities (JSON, CSV)")
    print(f"   ‚úÖ Admin setup and configuration")
    print(f"   ‚úÖ Sample jobs and documentation included")
    
    print(f"\nüéâ Demo completed! Access the full interface via:")
    print(f"   Dashboard ‚Üí üï∑Ô∏è Scraping Control Panel")
    print(f"   Login: admin/admin123")
    print(f"   URL: http://localhost:8502")
    
    print(f"\n" + "="*80)

if __name__ == "__main__":
    asyncio.run(demo_scraping_control_panel())
